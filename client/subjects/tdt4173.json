{"questions": [{"answer": " $v_{NB} = \\underset{v_j \\in V}{argmax} ~P(v_j) \\prod_{i} P(a_j|v_j)$", "question": "What is the formula for the Naive Bayes Classifier"}, {"answer": " A computer is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.", "question": "Define a well-posed learing problem"}, {"answer": " Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples.", "question": "What is the inductive learning hypothesis"}, {"answer": " $Consistent(h,D) \\equiv (\\forall \\big< x, c(x) \\big> \\in D) h(x) = c(x)$", "question": "A hypothesis $h$ is consistent with a set of training examples $D$ iff"}, {"answer": " $VS_{HD} \\equiv \\big\\{ h \\in H | Consistent(h,D) \\big\\}$", "question": "Definition of version space ($VS_{H,D}$)"}, {"answer": " Candidate Elimination and Find-S", "question": "Which of the following has an inductive bias? Rote Learner, Candidate Elimination or Find-S"}, {"answer": " Calculate $Q(h^{'}|h)$ using the current hypothesis $h$ and the observed data $X$ to estimate the probability distribution over Y. <br> $Q(h^{'}|h) \\leftarrow E[ln ~P(Y|h^{'})|h, X]$", "question": "What's the E-step in the EM-algorithm?"}, {"answer": " Replace the hypothesis $h$ by the hypothesis $h^{'}$ that maximizes the Q-function: <br> $h \\leftarrow \\underset{h^{'}}{argmax} ~Q(h^{'}|h)$", "question": "What's the M-step in the EM-algorithm?"}, {"answer": " Retrieve, Reuse, Revise and Retain", "question": "What's the four main phases of the CBR-cycle?"}, {"answer": " Weak classifiers are classifiers which perform only slightly better than a random classifier.", "question": "Define a weak classifier"}], "subject": "Machine Learning and Case-Based Reasoning", "code": "TDT4173"}