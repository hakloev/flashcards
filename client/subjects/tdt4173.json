{"questions": [{"answer": " All attributes are independent of each other. <br> $P(a_1 , a_2, \\dots , a_n |v_j) = \\prod_{i} P(a_i|v_j)$", "question": "What's the assumption of the Naive Bayes Classifier?"}, {"answer": " The EM-algorithm is used when we have a distribution of $k$ different normal distributuon, and we want to find the $k$ mean-values. It searches for the $h_{MAP}$ by repeatedly re-estimating the expected values of the hidden variables $z_{ij}$ given its current hypothesis $u_1, \\dots u_k$", "question": "When do we use the EM-algorithm?"}, {"answer": " $v_{NB} = \\underset{v_j \\in V}{argmax} ~P(v_j) \\prod_{i} P(a_j|v_j)$", "question": "What is the formula for the Naive Bayes Classifier?"}, {"answer": " Yes, but in order to learn the entire hypothesis space, you must apply as much data as there is in the whole world. Must know everything to learn everything.", "question": "Can we learn withot inductive bias?"}, {"answer": " Exploit the more-general-relationship to find the maximum specific hypothesis. Start with the most specific, and iterate every positive training example. For each attribute that is satisfied, do nothing. Else replace the attribute with the next more general constraint.", "question": "The main idea of the FIND-S algorithm is:"}, {"answer": " $Consistent(h,D) \\equiv (\\forall \\big< x, c(x) \\big> \\in D) h(x) = c(x)$", "question": "A hypothesis $h$ is consistent with a set of training examples $D$ iff:"}, {"answer": " A hypothesis overfitts a training set if another hypothesis has higher training error, but less error in the entire distribution $D$.", "question": "Give a textual description of overfitting:"}, {"answer": " $error_D (h) \\equiv \\underset{x\\in D}{Pr}[f(x) \\neq h(x)]$", "question": "Formula for the trye error:"}, {"answer": " Bagging is a collection of weak classifiers where each one of them gets data set of $m$ training examples drawn randomly with replacement from the original training set. The final classification is done by a majority vote of all the classifiers. The weak classifier should be unstable (small changes in training set, leads to large variations in classification).", "question": "What is 'bagging'?"}, {"answer": " If any instance classified as positive by a hypothesis $h_1$ is also classified positive by a more general hypothesis $h_2$, we have the more-general-relationship. It's written $h_j \\geq_{g} h_k$", "question": "Define the general-to-specific ordering of hypotheses:"}, {"answer": " Candidate Elimination and Find-S", "question": "Which of the following has an inductive bias? Rote Learner, Candidate Elimination or Find-S?"}, {"answer": " A computer is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.", "question": "Define a well-posed learing problem:"}, {"answer": " SVM and Decision Tree Learning", "question": "Give two algorithms that use a-posteriori knowledge:"}, {"answer": " $error_s (h) \\equiv \\frac{1}{n}\\sum_{x\\in S}\\gamma(f(x), h(x))$", "question": "Formula for the sample error:"}, {"answer": " Stop growing when data split is not statistically significant. Secondly grow the full tree, then post prune the tree.", "question": "Two methods for avoiding overfitting in ID3?"}, {"answer": " Bayesian Network and", "question": "Give two algorithms that use a-priori knowledge:"}, {"answer": " In order to solve the problem in a higher dimension it's defined a $K(x_i, x_j) = \\theta (x_i)^{T} \\theta (x_j)$. The different kernels have different properties, and finding the right for your problem is a difficult task.", "question": "What is a 'kernel' in SVM?"}, {"answer": " $VS_{HD} \\equiv \\big\\{ h \\in H | Consistent(h,D) \\big\\}$", "question": "Definition of version space ($VS_{H,D}$):"}, {"answer": " Calculate $Q(h^{'}|h)$ using the current hypothesis $h$ and the observed data $X$ to estimate the probability distribution over Y. <br> $Q(h^{'}|h) \\leftarrow E[ln ~P(Y|h^{'})|h, X]$", "question": "What's the E-step in the EM-algorithm?"}, {"answer": " Replace the hypothesis $h$ by the hypothesis $h^{'}$ that maximizes the Q-function: <br> $h \\leftarrow \\underset{h^{'}}{argmax} ~Q(h^{'}|h)$", "question": "What's the M-step in the EM-algorithm?"}, {"answer": " $error_{s}(h) < error_{s}(h^{'})$ and $error_D (h) > error_D (h^{'})$", "question": "What is the definition of overfitting? (the 'formula')"}, {"answer": " Cross-validation is to leave out a piece of the training set, and use it to check if their classified correct after training.", "question": "What is cross-validation?"}, {"answer": " Restriction bias, that is: a restriction in the hypothesis space. <br> Preference bias, that is: Like in ID3, we prefer small trees and high information gain close to the root.", "question": "There are two types of inductive bias, name them:"}, {"answer": " Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples.", "question": "What is the inductive learning hypothesis?"}, {"answer": " $h_{ML} \\equiv \\underset{h \\in H}{argmax}~ P(D|h)$", "question": "Mathematical definition of $h_{ML}$:"}, {"answer": " $\\big< ?,?,?,?,?,? \\big>$", "question": "The most general hypothesis is:"}, {"answer": " Retrieve, Reuse, Revise and Retain", "question": "What's the four main phases of the CBR-cycle?"}, {"answer": " $\\big< \\emptyset,\\emptyset,\\emptyset,\\emptyset,\\emptyset,\\emptyset \\big>$", "question": "The most specific hypothesis is:"}, {"answer": " It's restrictive. I many cases the attributes can be said to be dependent (e.g. thunder and rain)", "question": "Weakness of the Naive Bayes assumption?"}, {"answer": " Weak classifiers are classifiers which perform only slightly better than a random classifier.", "question": "Define a weak classifier:"}, {"answer": " $h_{MAP} = \\underset{h \\in H}{argmax}~ P(D|h)P(h)$", "question": "Mathematical definition of $h_{MAP}$:"}, {"answer": " The supporting planes are pushed apart until they bump into the support vectors.", "question": "SVM are called 'large margin classifiers', why?"}], "code": "TDT4173", "subject": "Machine Learning and Case-Based Reasoning"}