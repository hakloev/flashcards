Machine Learning and Case-Based Reasoning
TDT4173
What is the formula for the Naive Bayes Classifier?:= $v_{NB} = \underset{v_j \in V}{argmax} ~P(v_j) \prod_{i} P(a_j|v_j)$
Define a well-posed learing problem::= A computer is said to learn from experience $E$ with respect to some class of tasks $T$ and performance measure $P$, if its performance at tasks in $T$, as measured by $P$, improves with experience $E$.
What is the inductive learning hypothesis?:= Any hypothesis found to approximate the target function well over a sufficiently large set of training examples will also approximate the target function well over other unobserved examples.
A hypothesis $h$ is consistent with a set of training examples $D$ iff::= $Consistent(h,D) \equiv (\forall \big< x, c(x) \big> \in D) h(x) = c(x)$
Definition of version space ($VS_{H,D}$)::= $VS_{HD} \equiv \big\{ h \in H | Consistent(h,D) \big\}$
Which of the following has an inductive bias? Rote Learner, Candidate Elimination or Find-S?:= Candidate Elimination and Find-S
What's the E-step in the EM-algorithm?:= Calculate $Q(h^{'}|h)$ using the current hypothesis $h$ and the observed data $X$ to estimate the probability distribution over Y. <br> $Q(h^{'}|h) \leftarrow E[ln ~P(Y|h^{'})|h, X]$
What's the M-step in the EM-algorithm?:= Replace the hypothesis $h$ by the hypothesis $h^{'}$ that maximizes the Q-function: <br> $h \leftarrow \underset{h^{'}}{argmax} ~Q(h^{'}|h)$
What's the four main phases of the CBR-cycle?:= Retrieve, Reuse, Revise and Retain
Define a weak classifier::= Weak classifiers are classifiers which perform only slightly better than a random classifier.
The most general hypothesis is::= $\big< ?,?,?,?,?,? \big>$
The most specific hypothesis is::= $\big< \emptyset,\emptyset,\emptyset,\emptyset,\emptyset,\emptyset \big>$
Define the general-to-specific ordering of hypotheses::= If any instance classified as positive by a hypothesis $h_1$ is also classified positive by a more general hypothesis $h_2$, we have the more-general-relationship. It's written $h_j \geq_{g} h_k$
The main idea of the FIND-S algorithm is::= Exploit the more-general-relationship to find the maximum specific hypothesis. Start with the most specific, and iterate every positive training example. For each attribute that is satisfied, do nothing. Else replace the attribute with the next more general constraint.
There are two types of inductive bias, name them::= Restriction bias, that is: a restriction in the hypothesis space. <br> Preference bias, that is: Like in ID3, we prefer small trees and high information gain close to the root.
Can we learn withot inductive bias?:= Yes, but in order to learn the entire hypothesis space, you must apply as much data as there is in the whole world. Must know everything to learn everything.
What is the definition of overfitting? (the 'formula'):= $error_{s}(h) < error_{s}(h^{'})$ and $error_D (h) > error_D (h^{'})$
Give a textual description of overfitting::= A hypothesis overfitts a training set if another hypothesis has higher training error, but less error in the entire distribution $D$.
Give two algorithms that use a-priori knowledge::= Bayesian Network and
Give two algorithms that use a-posteriori knowledge::= SVM and Decision Tree Learning
What's the assumption of the Naive Bayes Classifier?:= All attributes are independent of each other. <br> $P(a_1 , a_2, \dots , a_n |v_j) = \prod_{i} P(a_i|v_j)$
Weakness of the Naive Bayes assumption?:= It's restrictive. I many cases the attributes can be said to be dependent (e.g. thunder and rain)
When do we use the EM-algorithm?:= The EM-algorithm is used when we have a distribution of $k$ different normal distributuon, and we want to find the $k$ mean-values. It searches for the $h_{MAP}$ by repeatedly re-estimating the expected values of the hidden variables $z_{ij}$ given its current hypothesis $u_1, \dots u_k$
What is cross-validation?:= Cross-validation is to leave out a piece of the training set, and use it to check if their classified correct after training.
What is 'bagging'?:= Bagging is a collection of weak classifiers where each one of them gets data set of $m$ training examples drawn randomly with replacement from the original training set. The final classification is done by a majority vote of all the classifiers. The weak classifier should be unstable (small changes in training set, leads to large variations in classification).
SVM are called 'large margin classifiers', why?:= The supporting planes are pushed apart until they bump into the support vectors.
What is a 'kernel' in SVM?:= In order to solve the problem in a higher dimension it's defined a $K(x_i, x_j) = \theta (x_i)^{T} \theta (x_j)$. The different kernels have different properties, and finding the right for your problem is a difficult task.
Two methods for avoiding overfitting in ID3?:= Stop growing when data split is not statistically significant. Secondly grow the full tree, then post prune the tree.
Formula for the sample error::= $error_s (h) \equiv \frac{1}{n}\sum_{x\in S}\gamma(f(x), h(x))$
Formula for the true error::= $error_D (h) \equiv \underset{x\in D}{Pr}[f(x) \neq h(x)]$
Mathematical definition of $h_{MAP}$::= $h_{MAP} = \underset{h \in H}{argmax}~ P(D|h)P(h)$
Mathematical definition of $h_{ML}$::= $h_{ML} \equiv \underset{h \in H}{argmax}~ P(D|h)$
What's Bayes' Theorem?:= $P(h|D) = \frac{P(D|h)P(h)}{P(D)}$
The prior probability of $h$ is::= $P(h)$
The probability of observing $D$ in a world where $h$ holds is::= $P(D|h)$
Some post-prune techniques has been presented, explain reduced error pruning::= This approach consideres each node a candidate for pruning. It removes the entire subtree of a node, making the node a leaf node. Nodes are removed only if the resulting tree performs no worse than the original over the validation set. The pruning is greedy.
Some post-prune techniques has been presented, explain rule post-pruning::= This approach infers the DT from the training set (growing the tree until the training data fits as well as possible, while allowing overfitting). It then converts the tree to set of rules by creating a rule for each path from root to leaf. Then it sorts the rules by estimated accuracy and and prunes the rules whose removal does not worsen its estimated accuracy.
When is a class $C$ PAC-learnable by a learner $L$ with a hypothesis space $h$?:= When <br> $c\in C$ <br> distributuons $D$ over $X$ <br> $\epsilon$ such that $0 < \epsilon < 1/2$, and <br> $\delta$ such that $0 < \delta < 1/2$
The joint probability distribution over all variables in a Bayesian Network are::= $P(y_1, \dots ,y_n) = \prod_{i=1}^{n}P(y_i|parents(y_i))$
What's the Brute-Force Bayes Concept Learning? Pseudocode and description::= It's a straightforward concept learning algorithm that outputs the $h_{MAP}$ by iterating each $h \in H$ and calculate the posterior probability by Bayes' Theorem. Finally it outputs the one with the highes probability: <br> $h_{MAP} = \underset{h \in H}{argmax} P(h|D)$
What do we mean by "bias in the estimate" in terms of evaluating hypotheses?:= The observed accuracy of the learned hypothesis over the training examples is often a poor estimator of its accuracy in future examples. This yields an biased estimate.
What do we mean by "variance in the estimate" in terms of evaulating hypotheses?:= We need a larger test set, since the smaller test examples, the greater the expected variance.
What's the main idea behind Occam's Razor?:= Prefer the shortest hypothesis that fits the data. Said otherwise: maximize complexity and simplicity.
Give a textual definition of the inductive bias::= The inductive bias is the set of assumptions made by a learner to be able to generalize from training examples
Give the formal definition of inductive bias::= The inductive bias of $L$ is any minimal set of assertions $B$ such that for any target concept $c$ and corresponding training examples $D_c$: <br> $(\forall x_i \in X)[(B \land D_c \land X_i ) \vdash L(x_i , D_c)]$
The formal definition of general-to-specific ordering of hypotheses is true iff::= $(\forall x \in X)[(h_k (x) = 1) \rightarrow (h_j (x) = 1)]$
Give an example of a well-posed learning problem? (The answer will be an indication on how it should look like):= Task T: playing checkers <br> Performance measure P: percent of games won against opponents <br> Training experience E: playing practice games against itself
